\documentclass[12pt]{article}

\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{float}

\geometry{margin=1in}

\lstset{
    language=C,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    frame=single
}

\title{Assignment 4}
\author{Nirupam Das \\ 2024CSB108 \\ 4th Semester}
\date{\today}



\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Question 1}

\subsection{Problem Statement}

An online educational analytics platform conducts national-level mock examinations for various competitive tests. In each examination session, approximately 110,000 students participate simultaneously. Once the test concludes, the scores obtained by all students are collected and stored on the server in the form of an unsorted integer array.

To generate real-time performance analytics, such as identifying the central tendency of student performance, the platform needs to compute the median score as efficiently as possible. Since the dataset is large, the choice of algorithm has a significant impact on both execution time and memory utilization.

Write a C program to determine the median score using two different approaches, as described below:

\begin{enumerate}
    \item \textbf{Fixed Pivot-Based Approach:}  
    Employs a divide-and-conquer strategy in which the pivot is selected in a fixed manner.

    \item \textbf{Quickselect-Based Approach:}  
    Determines the median using the Quickselect algorithm, which finds the required order statistic without fully sorting the array.
\end{enumerate}

After implementing both methods, perform a comparative analysis of the two approaches by evaluating and discussing their:

\begin{enumerate}
    \item \textbf{Time Complexity}
    \begin{itemize}
        \item Best case
        \item Average case
        \item Worst case
    \end{itemize}

    \item \textbf{Space Complexity}
    \begin{itemize}
        \item Best case
        \item Average case
        \item Worst case
    \end{itemize}
\end{enumerate}

Finally, justify which specific approach is more suitable for handling large-scale, real-time examination data and explain your conclusion with appropriate reasoning.

\subsection{Code}
\begin{lstlisting}
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

typedef struct {
  double clk;
  float median;
} metric;

int *init(bool fill) {
  int *arr = (int *)malloc(sizeof(int) * 110000);

  if (!arr) {
    printf("Allocation failed.");
    return NULL;
  }

  if (fill) {
    int random_bias = (rand() % 41) - 20;

    for (int i = 0; i < 110000; i++) {
      int mark = rand() % 101;
      mark += random_bias;

      if (mark < 0) mark = 0;
      if (mark > 100) mark = 100;

      arr[i] = mark;
    }
  }

  return arr;
}

int *freeArr(int *arr) {
  if (arr) free(arr);
  return NULL;
}

void memCpy(int *destn, int *source) {
  if (destn && source)
    memcpy(destn, source, sizeof(int) * 110000);
  else
    printf("Invalid source or destination.\n");
}

int randPvt(int low, int high) {
  return low + rand() % (high - low + 1);
}

void swap(int *a, int *b) {
  int temp = *a;
  *a = *b;
  *b = temp;
}

int midPvt(int *arr, int low, int high) {
  int mid = low + (high - low) / 2;

  if ((arr[low] <= arr[mid] && arr[mid] <= arr[high]) ||
      (arr[high] <= arr[mid] && arr[mid] <= arr[low]))
    return mid;
  else if ((arr[mid] <= arr[low] && arr[low] <= arr[high]) ||
           (arr[high] <= arr[low] && arr[low] <= arr[mid]))
    return low;
  else
    return high;
}

int pivot_location(int *arr, int left, int right, bool useMedian3) {
  int p_index;

  if (!useMedian3)
    p_index = randPvt(left, right);      // Random pivot
  else
    p_index = midPvt(arr, left, right);  // Median-of-3 pivot

  swap(&arr[p_index], &arr[left]);

  int i = left;
  int j = right;

  while (i < j) {
    while (arr[i] <= arr[left] && i < right) i++;
    while (arr[j] > arr[left]) j--;

    if (i < j)
      swap(&arr[i], &arr[j]);
  }

  swap(&arr[left], &arr[j]);
  return j;
}

int quickSelect(int *arr, int low, int high, int val_index, bool useMedian3) {
  if (low == high)
    return arr[low];

  if (low < high) {
    int p_index = pivot_location(arr, low, high, useMedian3);

    if (p_index == val_index)
      return arr[p_index];
    else if (p_index > val_index)
      return quickSelect(arr, low, p_index - 1, val_index, useMedian3);
    else
      return quickSelect(arr, p_index + 1, high, val_index, useMedian3);
  }

  return -1;
}

void test(metric *m, int *arr, const char *method) {
  int *cpyArr = init(false);
  memCpy(cpyArr, arr);

  clock_t start, end;
  int mid1, mid2;

  if (!strcmp("Random Quick Select", method)) {
    start = clock();

    mid1 = quickSelect(cpyArr, 0, 110000 - 1, (110000 / 2) - 1, false);
    mid2 = quickSelect(cpyArr, 110000 / 2, 110000 - 1, 110000 / 2, false);

    m->median = (mid1 + mid2) * 0.5;

    end = clock();
  }
  else {
    start = clock();

    mid1 = quickSelect(cpyArr, 0, 110000 - 1, (110000 / 2) - 1, true);
    mid2 = quickSelect(cpyArr, 110000 / 2, 110000 - 1, 110000 / 2, true);

    m->median = (mid1 + mid2) * 0.5;

    end = clock();
  }

  m->clk = (double)(end - start) / CLOCKS_PER_SEC;

  freeArr(cpyArr);
}

int main() {
  srand(time(NULL));

  metric m = {0, 0};
  int *arr = init(true);

  printf("Finding median using Random Pivot Quick Select:\n");
  printf("-----------------------------------------------\n");
  test(&m, arr, "Random Quick Select");
  printf("\tMedian: %f\n\tRequired Time: %f\n\n", m.median, m.clk);

  printf("Finding median using Median-of-3 Quick Select:\n");
  printf("----------------------------------------------\n");
  test(&m, arr, "Median3 Quick Select");
  printf("\tMedian: %f\n\tRequired Time: %f\n", m.median, m.clk);

  freeArr(arr);
  return 0;
}

\end{lstlisting}

\subsection{Output}

\begin{figure}[H]
     \centering
    \includegraphics[width=0.7\textwidth]{Pasted image.png}
    \caption{Output: Question 1}   
\end{figure}

\subsection{Explanation}
Quick Select follows a divide-and-conquer approach similar to Quick Sort, but instead of recursively processing both partitions, it recurses into only one partition that contains the desired order statistic.
\paragraph{Partition Cost}
Each call to \texttt{pivot\_location()} scans the subarray once.  
Therefore, partitioning needs:

\[
\Theta(n)
\]

time for a subarray of size $n$.
\paragraph{Recurrence Relation}

Let $T(n)$ denote the time required to select the $k^{th}$ smallest element from $n$ elements.

\begin{enumerate}
    \item \textbf{Best Case:}  
    If the pivot divides the array into two nearly equal halves, then we process only 1 half:

    \[
    T(n) = T\left(\frac{n}{2}\right) + cn
    \]

    By solving we get:

    \[
    T(n) = cn + c\frac{n}{2} + c\frac{n}{4} + \dots
    \]

    This is a geometric series:

    \[
    T(n) = 2cn = \Theta(n)
    \]

    So, the best-case time complexity is:

    \[
    \boxed{\Theta(n)}
    \]

    \item \textbf{Average Case (Random Pivot):}  

    With random pivot selection, we can expect the partition to be reasonably balanced.  
    The expected recurrence is:

    \[
    T(n) = T\left(\alpha n\right) + cn
    \quad \text{where } 0 < \alpha < 1
    \]

    By solving:

    \[
    \boxed{\Theta(n)}
    \]

    Thus, Randomized Quick Select runs in expected linear time.

    \item \textbf{Worst Case:}  

    If the pivot is always the smallest or largest element:

    \[
    T(n) = T(n - 1) + cn
    \]

    Expanding,

    \[
    T(n) = cn + c(n-1) + c(n-2) + \dots + c
    \]

    \[
    = c \frac{n(n+1)}{2}
    \]

    \[
    = \Theta(n^2)
    \]

    Thus, worst-case time complexity:

    \[
    \boxed{\Theta(n^2)}
    \]

\end{enumerate}

\paragraph{Effect of Pivot}

\begin{itemize}
    \item \textbf{Random Pivot:}  
    Expected time complexity is $\Theta(n)$, but worst-case time complexity is still $\Theta(n^2)$.
    
    \item \textbf{Median-of-Three Pivot:}  
    This process reduces probability of worst-case partitions.  
    However, theoretical worst-case complexity is still:
    
    \[
    \Theta(n^2)
    \]
\end{itemize}

\subsubsection*{Space Complexity}

\paragraph{Auxiliary Space}

Quick Select is an in-place algorithm. No additional arrays are created during recursion.

Partitioning uses constant extra variables.
Therefore, auxiliary space per call is:

\[
\Theta(1)
\]

\paragraph{Recursive Stack Space}

\begin{itemize}
    \item Best/Average Case Depth:
    \[
    O(\log n)
    \]
    
    \item Worst Case Depth:
    \[
    O(n)
    \]
\end{itemize}

\paragraph{Total Space Complexity}

\[
\boxed{
\begin{cases}
O(\log n) & \text{(average case)} \\
O(n) & \text{(worst case)}
\end{cases}
}
\]

\section{Question 2}
\subsection{Problem Statement}
A national digital census and survey platform periodically gathers extensive
demographic and economic information from 120000 households distributed across
the country. Among the various data points collected, household income values are
stored on the server in the form of a large unsorted array. After the completion of each
survey cycle, policymakers and analysts require reliable statistical indicators to support
data-driven decision-making. One of the most critical measures is the median
household income, as it provides a robust representation of central tendency and is not
unduly influenced by extreme income values.\\

Given the enormous size of the dataset, sorting the entire array to compute the median
becomes computationally expensive and impractical for real-time or near real-time
analysis. To overcome this limitation, the analytics team opts for a deterministic
linear-time selection algorithm, commonly referred to as the Median of Medians
method, which guarantees O(n) worst-case time complexity.\\

Write a C program that computes the median household income using the Median
of Medians algorithm. The implementation should avoid full-array sorting and ensure
deterministic performance irrespective of input distribution.

\subsection{Code}
\begin{lstlisting}

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

int *init(int n) {
    int *arr = (int *)malloc(sizeof(int) * n);
    if (!arr) {
        printf("Memory allocation failed.\n");
        exit(1);
    }
    return arr;
}

void swap(int *a, int *b) {
    int temp = *a;
    *a = *b;
    *b = temp;
}


int partitionQS(int *arr, int low, int high) {
    int pivot = arr[high];
    int i = low;

    for (int j = low; j < high; j++) {
        if (arr[j] < pivot) {
            swap(&arr[i], &arr[j]);
            i++;
        }
    }
    swap(&arr[i], &arr[high]);
    return i;
}

int quickSelect(int *arr, int low, int high, int k) {
    if (low <= high) {
        int pivotIndex = partitionQS(arr, low, high);

        if (pivotIndex == k)
            return arr[pivotIndex];
        else if (pivotIndex > k)
            return quickSelect(arr, low, pivotIndex - 1, k);
        else
            return quickSelect(arr, pivotIndex + 1, high, k);
    }
    return -1;
}


void insertionSort(int *arr, int n) {
    for (int i = 1; i < n; i++) {
        int key = arr[i];
        int j = i - 1;

        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j--;
        }
        arr[j + 1] = key;
    }
}


int partition(int *arr, int low, int high, int pivot) {
    for (int i = low; i <= high; i++) {
        if (arr[i] == pivot) {
            swap(&arr[i], &arr[low]);
            break;
        }
    }

    int i = low;
    int j = high;

    while (i < j) {
        while (arr[i] <= pivot && i < high) i++;
        while (arr[j] > pivot) j--;

        if (i < j)
            swap(&arr[i], &arr[j]);
    }

    swap(&arr[low], &arr[j]);
    return j;
}

int medianOfMedians(int *arr, int low, int high, int k) {
    if (low == high)
        return arr[low];

    int n = high - low + 1;
    int numGroups = (n + 4) / 5;
    int medians[numGroups];

    for (int i = 0; i < numGroups; i++) {
        int subLow = low + i * 5;
        int subHigh = subLow + 4;
        if (subHigh > high)
            subHigh = high;

        int size = subHigh - subLow + 1;
        insertionSort(&arr[subLow], size);
        medians[i] = arr[subLow + size / 2];
    }

    int medianPivot;
    if (numGroups == 1)
        medianPivot = medians[0];
    else
        medianPivot = medianOfMedians(medians, 0, numGroups - 1, numGroups / 2);

    int pivotIndex = partition(arr, low, high, medianPivot);
    int rank = pivotIndex - low;

    if (rank == k)
        return arr[pivotIndex];
    else if (rank > k)
        return medianOfMedians(arr, low, pivotIndex - 1, k);
    else
        return medianOfMedians(arr, pivotIndex + 1, high, k - rank - 1);
}


int main() {

    srand(time(NULL));

    FILE *fp = fopen("results.txt", "w");
    fprintf(fp, "# n QS_best QS_avg QS_worst MoM_best MoM_avg MoM_worst\n");

    int sizes[] = {20000, 50000, 80000, 120000, 200000};

    int tests = 5;

    for (int s = 0; s < tests; s++) {

        int n = sizes[s];
        int k = n / 2;


        int *qs_best = init(n);
        int *qs_avg = init(n);
        int *qs_worst = init(n);

        int *mom_best = init(n);
        int *mom_avg = init(n);
        int *mom_worst = init(n);

        for (int i = 0; i < n; i++) {
            qs_worst[i] = i;
            mom_worst[i] = i;
        }

        // Best Case (Median at Last)

        for (int i = 0; i < n; i++) {
            qs_best[i] = i;
            mom_best[i] = i;
        }

        swap(&qs_best[n/2], &qs_best[n-1]);
        swap(&mom_best[n/2], &mom_best[n-1]);

        // Average Case (Random) 

        for (int i = 0; i < n; i++) {
            int val = rand();
            qs_avg[i] = val;
            mom_avg[i] = val;
        }

        clock_t start, end;

        start = clock();
        quickSelect(qs_best, 0, n - 1, k);
        end = clock();
        double qs_best_time = (double)(end - start) / CLOCKS_PER_SEC;

        start = clock();
        quickSelect(qs_avg, 0, n - 1, k);
        end = clock();
        double qs_avg_time = (double)(end - start) / CLOCKS_PER_SEC;

        start = clock();
        quickSelect(qs_worst, 0, n - 1, k);
        end = clock();
        double qs_worst_time = (double)(end - start) / CLOCKS_PER_SEC;

        start = clock();
        medianOfMedians(mom_best, 0, n - 1, k);
        end = clock();
        double mom_best_time = (double)(end - start) / CLOCKS_PER_SEC;

        start = clock();
        medianOfMedians(mom_avg, 0, n - 1, k);
        end = clock();
        double mom_avg_time = (double)(end - start) / CLOCKS_PER_SEC;

        start = clock();
        medianOfMedians(mom_worst, 0, n - 1, k);
        end = clock();
        double mom_worst_time = (double)(end - start) / CLOCKS_PER_SEC;

        printf("n=%d\n", n);
        printf("QS -> Best:%f Avg:%f Worst:%f\n",
               qs_best_time, qs_avg_time, qs_worst_time);
        printf("MoM-> Best:%f Avg:%f Worst:%f\n\n",
               mom_best_time, mom_avg_time, mom_worst_time);

        fprintf(fp, "%d %f %f %f %f %f %f\n",
                n,
                qs_best_time, qs_avg_time, qs_worst_time,
                mom_best_time, mom_avg_time, mom_worst_time);

        free(qs_best);
        free(qs_avg);
        free(qs_worst);
        free(mom_best);
        free(mom_avg);
        free(mom_worst);
    }

    fclose(fp);
    return 0;
}


\end{lstlisting}

\subsection{Output}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Pasted image (2).png}
    \caption{Question 2 Output}
\end{figure}

\subsection{GNU-PLOT Script}
\begin{verbatim}
set terminal png size 1024,768
set output "comparison.png"

set title "QuickSelect vs Median of Medians"
set xlabel "Input Size (n)"
set ylabel "Execution Time (seconds)"
set grid
set key left top

plot "results.txt" using 1:2 with linespoints title "QuickSelect", \
     "results.txt" using 1:3 with linespoints title "MedianOfMedians"

\end{verbatim}

\subsection{Time complexity plot}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{comparison.png}
    \caption{Question 2 time complexity plot}
\end{figure}

\subsection{Explanation}


\subsubsection{Performance Analysis}

Quickselect uses a partition-based approach , while Median of Medians ensures a balanced partition by selecting a pivot near the true median.

\paragraph{i) Time Complexity Analysis}\mbox{}\\

\textbf{1. Quickselect Algorithm}
\begin{itemize}
    \item \textbf{Best \& Average Case:} The pivot partitions the array roughly in half.
    \[ T(n) = T(n/2) + O(n) \implies O(n) \]
    \item \textbf{Worst Case:} Occurs when the array is already sorted, and the pivot (last element) creates a highly unbalanced partition ($n-1$ vs $0$).
    \[ T(n) = T(n-1) + O(n) \implies O(n^2) \]
\end{itemize}

\textbf{2. Median of Medians Algorithm}
Median of Medians divides the array into groups of 5 and finds the median of each to select a pivot. This guarantees the pivot is within the $30^{th}$ to $70^{th}$ percentile.
\begin{itemize}
    \item \textbf{Recurrence Relation (All Cases):}
    \[ T(n) \le T\left(\lceil \frac{n}{5} \rceil\right) + T\left(\frac{7n}{10}\right) + O(n) \]
    Here, $T(n/5)$ is the cost to find the median of medians, and $T(7n/10)$ represents the worst-case recursive step on the remaining elements.
    \item \textbf{Complexity:} Since the sum of fractions $\frac{1}{5} + \frac{7}{10} = 0.9 < 1$, the work decreases geometrically.
    \[ \text{Best, Average, and Worst Case: } O(n) \]
\end{itemize}

\paragraph{ii) Space Complexity Analysis}

\begin{itemize}
    \item \textbf{Quickselect:} In the worst case (sorted input), the recursion stack depth reaches $O(n)$. In the average case, it is $O(\log n)$.
    \item \textbf{Median of Medians:} Due to guaranteed balanced partitioning, the recursion depth is bounded logarithmically in all cases.
    \[ \text{Space Complexity: } O(\log n) \]
\end{itemize}

\subsubsection{Suitability for Large-Scale Census Data}

Large-scale census datasets always halves massive volume ($N > 10^6$) and may be partially sorted (e.g. sorted by region ID or timestamps).

\begin{enumerate}
    \item \textbf{Quickselect:} Althpugh Quickselect has smaller constant factors and is faster on random data, it is highly unsuitable for census data if implemented with a simple deterministic pivot (e.g., `arr[high]`). Pre-sorted census data causes the $O(n^2)$ worst-case behavior, potentially causing stack overflow due to long recursion tree.
    \item \textbf{Stability of Median of Medians:} The Median of Medians method provides a strict $O(n)$ upper bound regardless of input structure. Although it has higher constant factors per iteration, it is the best choice to ensure predictable performance on large, potentially sorted datasets where worst-case reliability is critical.
\end{enumerate}

\clearpage
\section{Question 3}
\subsection{Problem Statement}
A financial technology analytics firm processes transaction data from two
independent banking systems. Each bank stores the daily transaction amounts of its
customers in a sorted array (sorted in non-decreasing order). At the end of each day,
the firm must compute the median transaction value across both banks to generate
risk and liquidity assessment reports. Due to strict performance requirements and
memory constraints, the firm cannot afford to merge the two arrays into a single array.
Instead, the median must be computed directly from the two sorted datasets using an
efficient algorithm.

Write a C program to compute the \textbf{median of the combined dataset} formed by \textbf{two given
sorted arrays, without merging the arrays into a single structure}. Further, examine
whether it is possible to design an algorithm that solves this problem with \textbf{logarithmic
time complexity}. If such an algorithm exists, implement your solution strictly
following that approach and justify its efficiency through appropriate time complexity
analysis.

\subsection{Code}
\begin{lstlisting}
#include <stdio.h>
#include <stdlib.h>
#include <limits.h>

int* init(int size) {
    int* arr = (int*)malloc(size * sizeof(int));
    if (arr == NULL) {
        printf("Memory allocation failed!\n");
        exit(1);
    }
    return arr;
}

int* freeArr(int* arr) {
    if (arr != NULL) {
        free(arr);
        arr = NULL;
    }
    return arr;
}


void swap(int* a, int* b) {
    int temp = *a;
    *a = *b;
    *b = temp;
}


void insertionSort(int* arr, int n) {
    for (int i = 1; i < n; i++) {
        int key = arr[i];
        int j = i - 1;

        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j--;
        }
        arr[j + 1] = key;
    }
}


double findMed(int *A, int n, int *B, int m) {

    if (n > m)
        return findMed(B, m, A, n);

    int low = 0, high = n;
    int total = n + m;
    int half = (total + 1) / 2;

    while (low <= high) {

        int i = (low + high) / 2;
        int j = half - i;

        int Aleft  = (i == 0) ? INT_MIN : A[i - 1];
        int Aright = (i == n) ? INT_MAX : A[i];

        int Bleft  = (j == 0) ? INT_MIN : B[j - 1];
        int Bright = (j == m) ? INT_MAX : B[j];

        if (Aleft <= Bright && Bleft <= Aright) {

            if (total % 2 == 0) {
                int leftMax  = (Aleft > Bleft) ? Aleft : Bleft;
                int rightMin = (Aright < Bright) ? Aright : Bright;
                return (leftMax + rightMin) / 2.0;
            } else {
                return (Aleft > Bleft) ? Aleft : Bleft;
            }
        }
        else if (Aleft > Bright)
            high = i - 1;
        else
            low = i + 1;
    }

    return -1;
}

int main() {

    int n, m;

    printf("Enter size of first array: ");
    scanf("%d", &n);

    printf("Enter size of second array: ");
    scanf("%d", &m);

    int* A = init(n);
    int* B = init(m);

    printf("Enter elements of first array:\n");
    for (int i = 0; i < n; i++)
        scanf("%d", &A[i]);

    printf("Enter elements of second array:\n");
    for (int i = 0; i < m; i++)
        scanf("%d", &B[i]);

    insertionSort(A, n);
    insertionSort(B, m);

    double median = findMed(A, n, B, m);

    printf("Median of combined arrays = %.2f\n", median);

    A = freeArr(A);
    B = freeArr(B);

    return 0;
}

\end{lstlisting}

\subsection{Output}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Pasted image (3).png}
    \caption{Question 3- Output}
\end{figure}

\subsection{Explanation}

Instead of merging the both array and sorting (which requires 
$O(n+m)$ time and space), here we are applying binary search
on the smaller array. We have to determine a partition such that:

\begin{itemize}
    \item The left partition contains exactly half of the total elements.
    \item Every element in the left partition is less than or equal to every element in the right partition.
\end{itemize}

Once this condition is satisfied, the median is obtained directly from the 
boundary elements of the partitions.

\subsubsection*{Time Complexity}

Let $n$ and $m$ be the sizes of the two arrays, with $n \le m$. 
Binary search is performed on the smaller array.

\textbf{Best Case:}
\[
T(n) = O(1)
\]

\textbf{Average Case:}
\[
T(n) = T(n/2) + O(1)
\]
\[
\Rightarrow T(n) = O(\log n)
\]

\textbf{Worst Case:}
\[
T(n) = O(\log n)
\]

Thus, the overall time complexity is:
\[
O(\log(\min(n,m)))
\]

\subsubsection*{Space Complexity}

The algorithm uses only constant auxiliary variables and does not allocate 
additional data structures.

\[
\text{Best Case} = O(1)
\]
\[
\text{Average Case} = O(1)
\]
\[
\text{Worst Case} = O(1)
\]

\subsubsection*{Conclusion}

Compared to the merging approach that will require $O(n+m)$ time and space, 
the binary-search based method achieves logarithmic time and constant 
space complexity. Therefore, it is suitable for 
large-scale financial data processing systems where memory and 
performance constraints important.


\end{document}
